\documentclass[10pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx,listings}
\setlength{\parindent}{0pt}

\begin{document}

\begin{center}
{\Large \textbf{Homework 2: Sentiment}}

\begin{tabular}{rl}
\\
Course: & CS 221 Spring 2019 \\
Name: & Bryan Yaggi
\end{tabular}
\end{center}

\section*{\normalsize Problem 1: Building Intuition}

Rotten Tomatoes has classified these reviews as "positive" and "negative", respectively, as indicated by the intact tomato on the left and the splattered tomato on the right. In this assignment, you will create a simple text classification system that can perform this task automatically. We'll warm up with the following set of four mini-reviews, each either labeled positive $(+1)$ or negative $(-1)$:

	\begin{enumerate}
		\item $(-1)$ pretty bad
		\item $(+1)$ good plot
		\item $(-1)$ not good
		\item $(+1)$ pretty scenery
	\end{enumerate}
	
Each review $x$ is mapped onto a feature vector $\phi(x)$, which maps each word to the number of occurrences of that word in the review. For example, the first review maps to the (sparse) feature vector $\phi(x)=\{pretty:1,bad:1\}$. Recall the definition of the hinge loss:
$$Loss_{hinge}(x, y, \mathbf{w}) = max\{ 0, 1 - \mathbf{w} \cdot \phi(x)y \},$$

where $y$ is the correct label.

\begin{enumerate}[label=(\alph*)]

  \item Suppose we run stochastic gradient descent, updating the weights according to
$$w \leftarrow w - \eta \nabla_w Loss_{hinge} (x, y, \mathbf{w}),$$
once for each of the four examples in the order given above. After the classifier is trained on the given four data points, what are the weights of the six words ("pretty", "good", "bad", "plot", "not", "scenery") that appear in the above reviews? Use $\eta =.5$ as the step size and initialize $w = [0, \dots, 0]$. Assume that $\nabla_w Loss_{hinge} (x, y, \mathbf{w})=0$ when the margin is exactly 1. ``"

Let the feature and weights vector be the number of times ``pretty", ``bad", ``bad", ``plot", ``not", and ``scenery" occur in $x$, respectively.
  
  \begin{align*}
  x_1 &= ``pretty\ bad", \phi(x_1) = \{ "pretty" = 1, "bad" = 1 \}, y_1 = -1\\
  x_2 &=``good\ plot", \phi(x_2) = \{ "good" = 1, "plot" = 1 \}, y_2 = +1\\
  x_3 &=``not\ good", \phi(x_3) = \{ "not" = 1, "good" = 1 \}, y_3 = -1\\
  x_4 &= ``pretty\ scenery", \phi(x_4) = \{ "pretty" = 1, "scenery" = 1 \}, y_4 = +1\\
  \end{align*}
  $$
  \frac{\partial Loss_{hinge}}{\partial w} = \begin{cases}
  -\phi(x)y, &w \cdot \phi(x)y < 1\\
  0, &w \cdot \phi(x)y \geq 1\\
  \end{cases}
  $$
  \begin{align*}
  w_{after\ step\ 1} &= \begin{bmatrix}
  0\\
  0\\
  0\\
  0\\
  0\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  (-1)(-1)\\
  0\\
  (-1)(-1)\\
  0\\
  0\\
  0\\
  \end{bmatrix} = \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  0\\
  0\\
  0\\
  \end{bmatrix}\\  
  w_{after\ step\ 2} &= \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  0\\
  0\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  0\\
  (-1)(1)\\
  0\\
  (-1)(1)\\
  0\\
  0\\
  \end{bmatrix} = \begin{bmatrix}
  -.5\\
  .5\\
  -.5\\
  .5\\
  0\\
  0\\
  \end{bmatrix}\\
  w_{after\ step\ 3} &= \begin{bmatrix}
  -.5\\
  .5\\
  -.5\\
  .5\\
  0\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  0\\
  (-1)(-1)\\
  0\\
  0\\
  (-1)(-1)\\
  0\\
  \end{bmatrix} = \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  .5\\
  -.5\\
  0\\
  \end{bmatrix}\\
  w_{after\ step\ 4} &= \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  .5\\
  -.5\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  (-1)(1)\\
  0\\
  0\\
  0\\
  0\\
  (-1)(1)\\
  \end{bmatrix} = \begin{bmatrix}
  0\\
  0\\
  -.5\\
  .5\\
  -.5\\
  .5\\
  \end{bmatrix}
  \end{align*}
  
  \item Create a small labeled dataset of four mini-reviews using the words "not", "good", and "bad", where the labels make intuitive sense. Each review should contain one or two words, and no repeated words. Prove that no linear classifier using word features can get zero error on your dataset. Remember that this is a question about classifiers, not optimization algorithms; your proof should be true for any linear classifier, regardless of how the weights are learned.
After providing such a dataset, propose a single additional feature that we could augment the feature vector with that would fix this problem. (Hint: think about the linear effect that each feature has on the classification score.)

Let $\phi_1, \phi_2, \phi_3$ be the number of times ``bad", ``good", and ``not" occur in $x$, respectively.  
  
  \begin{align*}
  x_1 &= ``bad", \phi_1 = \{ "bad" = 1 \}, y_1 = -1\\
  x_2 &=``good", \phi_2 = \{ "good" = 1 \}, y_2 = +1\\
  x_3 &=``not\ bad", \phi_3 = \{ "not" = 1, "bad" = 1 \}, y_3 = +1\\
  x_4 &= ``not\ good", \phi_4 = \{ "not" = 1, "good" = 1 \}, y_4 = -1\\
  \end{align*}
  \begin{align*}
  \begin{bmatrix}
  1 & 0 & 0\\
  0 & 1 & 0\\
  1 & 0 & 1\\
  0 & 1 & 1\\
  \end{bmatrix} \cdot \begin{bmatrix}
  w_1\\
  w_2\\
  w_3\\
  \end{bmatrix} - \begin{bmatrix}
  -1\\
  1\\
  1\\
  -1\\
  \end{bmatrix} \neq 0
  \end{align*}
  
Add a feature for word count. The feature will be 0 if $x$ has one word, and 1 if $x$ has two words.

	\begin{align*}
  x_1 &= ``bad", \phi_1 = \{ "bad" = 1 \}, y_1 = -1\\
  x_2 &=``good", \phi_2 = \{ "good" = 1 \}, y_2 = +1\\
  x_3 &=``not\ bad", \phi_3 = \{ "not" = 1, "bad" = 1, count = 1 \}, y_3 = +1\\
  x_4 &= ``not\ good", \phi_4 = \{ "not" = 1, "good" = 1, count = 1 \}, y_4 = -1\\
  \end{align*}
  \begin{align*}
  \begin{bmatrix}
  1 & 0 & 0 & 0\\
  0 & 1 & 0 & 0\\
  1 & 0 & 1 & 1\\
  0 & 1 & 1 & 1\\
  \end{bmatrix} \cdot \begin{bmatrix}
  w_1\\
  w_2\\
  w_3\\
  w_4\\
  \end{bmatrix} - \begin{bmatrix}
  -1\\
  1\\
  1\\
  -1\\
  \end{bmatrix} &= 0\\
  \begin{bmatrix}
  w_1\\
  w_2\\
  w_3\\
  w_4\\
  \end{bmatrix} &= \begin{bmatrix}
  1\\
  -1\\
  1\\
  -1\\
  \end{bmatrix} or \begin{bmatrix}
  1\\
  -1\\
  -1\\
  1\\
  \end{bmatrix}
  \end{align*}

\end{enumerate}


\section*{\normalsize Problem 2: Predicting Movie Ratings}

Suppose that we are now interested in predicting a numeric rating for each movie review. We will use a non-linear predictor that takes a movie review $x$ and returns $\sigma(\mathbf{w} \cdot \phi(x))$, where $\sigma(z)=(1+e^{-z})^{-1}$ is the logistic function that squashes a real number to the range $(0,1)$. Suppose that we wish to use the squared loss. For this problem, assume that the movie rating $y$ is a real-valued variable in the range $[0,1]$.

\begin{enumerate}[label=(\alph*)]

  \item Write out the expression for $Loss(x, y, \mathbf{w})$.
  
  \begin{align*}
  Loss_{squared} (x, y, \mathbf{w}) &= (f_w(x) - y)^2\\
  &= (\sigma(\mathbf{w} \cdot \phi(x)) - y)^2\\
  &= ((1 + e^{-w \cdot \phi(x)})^{-1} - y)^2
  \end{align*}
  
  \item Compute the gradient of the loss with respect to $w$. Hint: you can write the answer in terms of the predicted value $p = \sigma(\mathbf{w} \cdot \phi(x))$.
  
  \begin{align*}
  \nabla_w Loss_{squared} (x, y, \mathbf{w}) &= 2(p - y) \frac{\partial p}{\partial \mathbf{w}}\\
  &= 2((1 + e^{-w \cdot \phi(x)})^{-1} - y)(-(1 + e^{-w \cdot \phi(x)})^{-2})(e^{-w \cdot \phi(x)})(-\phi(x))\\
  &= 2((1 + e^{-w \cdot \phi(x)})^{-1} - y)(1 + e^{-w \cdot \phi(x)})^{-2}e^{-w \cdot \phi(x)}\phi(x)
  \end{align*}
  
  \item Suppose there is one datapoint $(x, y)$ with some given $\phi(x)$ and $y = 1$. Can you choose a $\mathbf{w}$ to make the magnitude of the gradient of the loss with respect to $\mathbf{w}$ arbitrarily small (i.e., minimize the magnitude of the gradient and make it asymptotically approach some value)? If so, how small? Can the magnitude of the gradient ever be exactly zero? You are allowed to make the magnitude of $\mathbf{w}$ arbitrarily large. Hint: try to understand intuitively what is going on and the contribution of each part of the expression. If you find yourself doing too much algebra, you're probably doing something suboptimal. Motivation: the reason why we're interested in the magnitude of the gradients is because it governs how far gradient descent will step. For example, if the gradient is close to zero when $\mathbf{w}$ is very far from the optimum, then it could take a long time for gradient descent to reach the optimum (if at all). This is known as the vanishing gradient problem when training neural networks.
  
  \begin{align*}
  \lim_{w \to \infty} \nabla_w Loss_{squared} (x, y, \mathbf{w}) &= 0\\
  \text{since} \lim_{w \to \infty} e^{-w \cdot \phi(x)} &= 0\\
  \lim_{w \to -\infty} \nabla_w Loss_{squared} (x, y, \mathbf{w}) &= 0\\
  \text{since} \lim_{w \to -\infty} ((1 + e^{-w \cdot \phi(x)})^{-2} &= 0
  \end{align*}
  
  The gradient will asymptotically reach 0, but never truly be 0.
  
  \item  Assuming the same data point as above, what is the largest magnitude that the gradient can take? Leave your answer in terms of $\lVert \phi(x) \rVert$.

  \begin{align*}
  \lVert \nabla_w Loss_{squared} (x, y, \mathbf{w}) \rVert &= 2(p - y) p^2 (1 - p) \lVert \phi(x) \rVert\\
  \lVert \nabla_w Loss_{squared} (x, y = 1, \mathbf{w}) \rVert &= 2(p - 1) p^2 (1 - p) \lVert \phi(x) \rVert\\
  &= (-2p^4 + 4p^3 -2p^2) \lVert \phi(x) \rVert\\
  \nabla_w^2 L_{squared} (x, y = 1, \mathbf{w}) &= -8p^3 + 12p^2 - 4p = 0 \implies p = 1, \frac{1}{2}\\
  max(\lVert \nabla_w Loss_{squared} (x, y = 1, \mathbf{w}) \rVert) &= -\frac{1}{8} \lVert \phi(x) \rVert
  \end{align*}
  
  \item The problem with the loss function we have defined so far is that is it is non-convex, which means that gradient descent is not guaranteed to find the global minimum, and in general these types of problems can be difficult to solve. So let us try to reformulate the problem as plain old linear regression. Suppose you have a dataset $\mathbf{D}$ consisting of $(x,y)$ pairs, and that there exists a weight vector $\mathbf{w}$ that yields zero loss on this dataset. Show that there is an easy transformation to a modified dataset $\mathbf{D'}$ of $(x,y')$ pairs such that performing least squares regression (using a linear predictor and the squared loss) on $\mathbf{D'}$ converges to a vector $\mathbf{w^*}$ that yields zero loss on $\mathbf{D'}$. Concretely, write an expression for $y'$ in terms of $y$ and justify this choice. This expression should not be a function of $\mathbf{w}$.
  
  The logit function is the inverse of the sigmoid function and converts from $[0,1]$ to $[-\infty,+\infty]$.
  
  $$y' = logit(y) = log(\frac{y}{1 - y})$$

\end{enumerate}

\section*{\normalsize Problem 3: Sentiment Classification}

In this problem, we will build a binary linear classifier that reads movie reviews and guesses whether they are ``positive" or ``negative." In this problem, you must implement the functions without using libraries like Scikit-learn.

\begin{enumerate}[label=(\alph*)]

  \item coding
  \item coding
  \item coding
  
  \item When you run the grader.py on test case 3b-2, it should output a weights file and a error-analysis file. Look through some example incorrect predictions and for five of them, give a one-sentence explanation of why the classification was incorrect. What information would the classifier need to get these correct? In some sense, there's not one correct answer, so don't overthink this problem. The main point is to convey intuition about the problem.

	Review 1: home alone goes hollywood , a funny premise until the kids start pulling off stunts not even steven spielberg would know how to do . besides , real movie producers aren't this nice .\\
Truth: -1, Prediction: 1 [WRONG]

	It is tough to determine the sentiment of this review via individual words alone.

	Review 2: a perfectly competent and often imaginative film that lacks what little lilo \& stitch had in spades -- charisma .\\
Truth: 1, Prediction: -1 [WRONG]

	This review had several stop-words that were weighted heavily negative.

	Review 3: a heady , biting , be-bop ride through nighttime manhattan , a loquacious videologue of the modern male and the lengths to which he'll go to weave a protective cocoon around his own ego .\\
Truth: 1, Prediction: -1 [WRONG]

	The vocabulary in this review is advanced, so many of the important words did not have significant weights assigned to them. Also, there were many stop-words heavily weighted negative.

	Review 4: 'it's painful to watch witherspoon's talents wasting away inside unnecessary films like legally blonde and sweet home abomination , i mean , alabama . '\\
Truth: -1, Prediction: 1 [WRONG]

	Several of the movie title words were heavily weighted positive. ``Painful" was curiously weighted heavily positive.

	Review 5: dull , if not devoid of wit , this shaggy dog longs to frisk through the back alleys of history , but scarcely manages more than a modest , snoozy charm .\\
Truth: -1, Prediction: 1 [WRONG]

	Consideration of word proximity is important in this review. Phrases like ``devoid of wit", ``scarcely manages", and ``snoozy charm" should all be negative, but the end words are weighted heavily positive, which leads to the review's positive sentiment score.
	
	\item coding
	
	\item Run your linear predictor with feature extractor \texttt{extractCharacterFeatures}. Experiment with different values of n to see which one produces the smallest test error. You should observe that this error is nearly as small as that produced by word features. How do you explain this? Construct a review (one sentence max) in which character n-grams probably outperform word features, and briefly explain why this is so.
	
	The predictor worked best with n between 5-7. This value is large enough to capture word proximity and most short words. If n is smaller, the n-grams are meaningless and if they are larger, they would be too unique.
	
	This approach would work well on a review like ``It's not good." The n-gram ``notgood" would be weighted negative. 

\end{enumerate}

\end{document}
