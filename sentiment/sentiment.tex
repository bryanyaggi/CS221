\documentclass[10pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx,listings}
\setlength{\parindent}{0pt}

\begin{document}

\begin{center}
{\Large \textbf{Homework 2: Sentiment}}

\begin{tabular}{rl}
\\
Course: & CS 221 Spring 2019 \\
Name: & Bryan Yaggi
\end{tabular}
\end{center}

\section*{\normalsize Problem 1: Building Intuition}

Rotten Tomatoes has classified these reviews as "positive" and "negative", respectively, as indicated by the intact tomato on the left and the splattered tomato on the right. In this assignment, you will create a simple text classification system that can perform this task automatically. We'll warm up with the following set of four mini-reviews, each either labeled positive $(+1)$ or negative $(-1)$:

	\begin{enumerate}
		\item $(-1)$ pretty bad
		\item $(+1)$ good plot
		\item $(-1)$ not good
		\item $(+1)$ pretty scenery
	\end{enumerate}
	
Each review $x$ is mapped onto a feature vector $\phi(x)$, which maps each word to the number of occurrences of that word in the review. For example, the first review maps to the (sparse) feature vector $\phi(x)=\{pretty:1,bad:1\}$. Recall the definition of the hinge loss:
$$Loss_{hinge}(x, y, \mathbf{w}) = max\{ 0, 1 - \mathbf{w} \cdot \phi(x)y \},$$

where $y$ is the correct label.

\begin{enumerate}[label=(\alph*)]

  \item Suppose we run stochastic gradient descent, updating the weights according to
$$w \leftarrow w - \eta \nabla_w Loss_{hinge} (x, y, \mathbf{w}),$$
once for each of the four examples in the order given above. After the classifier is trained on the given four data points, what are the weights of the six words ("pretty", "good", "bad", "plot", "not", "scenery") that appear in the above reviews? Use $\eta =.5$ as the step size and initialize $w = [0, \dots, 0]$. Assume that $\nabla_w Loss_{hinge} (x, y, \mathbf{w})=0$ when the margin is exactly 1. ``"

Let the feature and weights vector be the number of times ``pretty", ``bad", ``bad", ``plot", ``not", and ``scenery" occur in $x$, respectively.
  
  \begin{align*}
  x_1 &= ``pretty\ bad", \phi(x_1) = \{ "pretty" = 1, "bad" = 1 \}, y_1 = -1\\
  x_2 &=``good\ plot", \phi(x_2) = \{ "good" = 1, "plot" = 1 \}, y_2 = +1\\
  x_3 &=``not\ good", \phi(x_3) = \{ "not" = 1, "good" = 1 \}, y_3 = -1\\
  x_4 &= ``pretty\ scenery", \phi(x_4) = \{ "pretty" = 1, "scenery" = 1 \}, y_4 = +1\\
  \end{align*}
  $$
  \frac{\partial Loss_{hinge}}{\partial w} = \begin{cases}
  -\phi(x)y, &w \cdot \phi(x)y < 1\\
  0, &w \cdot \phi(x)y \geq 1\\
  \end{cases}
  $$
  \begin{align*}
  w_{after\ step\ 1} &= \begin{bmatrix}
  0\\
  0\\
  0\\
  0\\
  0\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  (-1)(-1)\\
  0\\
  (-1)(-1)\\
  0\\
  0\\
  0\\
  \end{bmatrix} = \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  0\\
  0\\
  0\\
  \end{bmatrix}\\  
  w_{after\ step\ 2} &= \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  0\\
  0\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  0\\
  (-1)(1)\\
  0\\
  (-1)(1)\\
  0\\
  0\\
  \end{bmatrix} = \begin{bmatrix}
  -.5\\
  .5\\
  -.5\\
  .5\\
  0\\
  0\\
  \end{bmatrix}\\
  w_{after\ step\ 3} &= \begin{bmatrix}
  -.5\\
  .5\\
  -.5\\
  .5\\
  0\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  0\\
  (-1)(-1)\\
  0\\
  0\\
  (-1)(-1)\\
  0\\
  \end{bmatrix} = \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  .5\\
  -.5\\
  0\\
  \end{bmatrix}\\
  w_{after\ step\ 4} &= \begin{bmatrix}
  -.5\\
  0\\
  -.5\\
  .5\\
  -.5\\
  0\\
  \end{bmatrix} - .5 \begin{bmatrix}
  (-1)(1)\\
  0\\
  0\\
  0\\
  0\\
  (-1)(1)\\
  \end{bmatrix} = \begin{bmatrix}
  0\\
  0\\
  -.5\\
  .5\\
  -.5\\
  .5\\
  \end{bmatrix}
  \end{align*}
  
  \item Create a small labeled dataset of four mini-reviews using the words "not", "good", and "bad", where the labels make intuitive sense. Each review should contain one or two words, and no repeated words. Prove that no linear classifier using word features can get zero error on your dataset. Remember that this is a question about classifiers, not optimization algorithms; your proof should be true for any linear classifier, regardless of how the weights are learned.
After providing such a dataset, propose a single additional feature that we could augment the feature vector with that would fix this problem. (Hint: think about the linear effect that each feature has on the classification score.)

Let $\phi_1, \phi_2, \phi_3$ be the number of times ``bad", ``good", and ``not" occur in $x$, respectively.  
  
  \begin{align*}
  x_1 &= ``bad", \phi_1 = \{ "bad" = 1 \}, y_1 = -1\\
  x_2 &=``good", \phi_2 = \{ "good" = 1 \}, y_2 = +1\\
  x_3 &=``not\ bad", \phi_3 = \{ "not" = 1, "bad" = 1 \}, y_3 = +1\\
  x_4 &= ``not\ good", \phi_4 = \{ "not" = 1, "good" = 1 \}, y_4 = -1\\
  \end{align*}
  \begin{align*}
  \begin{bmatrix}
  1 & 0 & 0\\
  0 & 1 & 0\\
  1 & 0 & 1\\
  0 & 1 & 1\\
  \end{bmatrix} \cdot \begin{bmatrix}
  w_1\\
  w_2\\
  w_3\\
  \end{bmatrix} - \begin{bmatrix}
  -1\\
  1\\
  1\\
  -1\\
  \end{bmatrix} \neq 0
  \end{align*}
  
Add a feature for word count. The feature will be 0 if $x$ has one word, and 1 if $x$ has two words.

	\begin{align*}
  x_1 &= ``bad", \phi_1 = \{ "bad" = 1 \}, y_1 = -1\\
  x_2 &=``good", \phi_2 = \{ "good" = 1 \}, y_2 = +1\\
  x_3 &=``not\ bad", \phi_3 = \{ "not" = 1, "bad" = 1, count = 1 \}, y_3 = +1\\
  x_4 &= ``not\ good", \phi_4 = \{ "not" = 1, "good" = 1, count = 1 \}, y_4 = -1\\
  \end{align*}
  \begin{align*}
  \begin{bmatrix}
  1 & 0 & 0 & 0\\
  0 & 1 & 0 & 0\\
  1 & 0 & 1 & 1\\
  0 & 1 & 1 & 1\\
  \end{bmatrix} \cdot \begin{bmatrix}
  w_1\\
  w_2\\
  w_3\\
  w_4\\
  \end{bmatrix} - \begin{bmatrix}
  -1\\
  1\\
  1\\
  -1\\
  \end{bmatrix} &= 0\\
  \begin{bmatrix}
  w_1\\
  w_2\\
  w_3\\
  w_4\\
  \end{bmatrix} &= \begin{bmatrix}
  1\\
  -1\\
  1\\
  -1\\
  \end{bmatrix} or \begin{bmatrix}
  1\\
  -1\\
  -1\\
  1\\
  \end{bmatrix}
  \end{align*}

\end{enumerate}

\iffalse
\section*{\normalsize Problem 2: Complexity}

When designing algorithms, it's useful to be able to do quick back of the envelope calculations to see how much time or space an algorithm needs. Hopefully, you'll start to get more intuition for this by being exposed to different types of problems.

\begin{enumerate}[label=(\alph*)]

  \item Suppose we have an image of a human face consisting of $nxn$ pixels. In our simplified setting, a face consists of two eyes, two ears, one nose, and one mouth, each represented as an arbitrary axis-aligned rectangle (i.e. the axes of the rectangle are aligned with the axes of the image). As we'd like to handle Picasso portraits too, there are no constraints on the location or size of the rectangles. How many possible faces (choice of its component rectangles) are there? In general, we only care about asymptotic complexity, so give your answer in the form of $O(nc)$ or $O(cn)$ for some integer $c$.
  
  To define a rectange, two bounding horizontal axes and two bounding vertical axes can be selected. 6 rectangles need to be chosen to constitute a face. This is a combinatorics problem.
  
  \begin{align*}
  \left(\binom{n + 1}{2}^2\right)^6 = \left(\frac{(n + 1)!}{2!(n - 1)!}\right)^{12} = \left(\frac{(n + 1)(n)}{4}\right)^{12} \implies O(n^{24})
  \end{align*}
  
  \item Suppose we have an $nxn$ grid. We start in the upper-left corner (position $(1,1)$), and we would like to reach the lower-right corner (position $(n,n)$) by taking single steps down and right. Define a function $c(i,j)$ to be the cost of touching position $(i,j)$, and assume it takes constant time to compute. Note that $c(i,j)$ can be negative. Give an algorithm for computing the minimum cost in the most efficient way. What is the runtime (just give the big-O)?
  
  The problem can be solved using recursion and dynamic programming. The complexity is $O(n^2)$. The following is a code sample in Python.
  
  \begin{lstlisting}
  cache = {}
  def findMinCost(row, col):
  	if (row, col) in cache:
  		return cache[(row, col)]
  	if row == target[0] and col == target[1]:
  		result = c(row, col)
  	elif row == target[0]:
  		result = c(row, col) + findMinCost(row, col+1)
  	elif col == target[1]:
  		result = c(row, col) + findMinCost(row+1, col)
  	else:
  		result = c(row, col) + min(findMinCost(row+1, col),
  			findMinCost(row, col+1))
  		
  	cache[(row, col)] = result
  	return result
  \end{lstlisting}
  
  \item Suppose we have a staircase with $n$ steps (we start on the ground, so we need $n$ total steps to reach the top). We can take as many steps forward at a time, but we will never step backwards. How many ways are there to reach the top? Give your answer as a function of $n$. For example, if $n=3$, then the answer is 4. The four options are the following: (1) take one step, take one step, take one step (2) take two steps, take one step (3) take one step, take two steps (4) take three steps.
  
  The solution was found by experimentation. Let $f(n)$ be the number of ways for a staircase with $n$ stairs.
  
  \begin{align*}
  f(1) &= 1 &\text{[1]}\\
  f(2) &= 2 &\text{[2] or [1,1]}\\
  f(3) &= 4 &\text{[3] or [2,1] or [1,2] or [1,1,1]}\\
  f(4) &= 8 &\text{[4] or [3,1] or [1,3] or [2,2] or [2,1,1] or [1,2,1] or [1,1,2] or [1,1,1,1]}\\
  f(n) &= 2^{n-1}
  \end{align*}
  
  \item Consider the scalar-valued function $f(w)$ from Problem 1f. Devise a strategy that first does preprocessing in $O(nd^2)$ time, and then for any given vector $w$, takes $O(d^2)$ time instead to compute $f(w)$.
Hint: Refactor the algebraic expression; this is a classic trick used in machine learning. Again, you may find it helpful to work out the scalar case first.

  \begin{align*}
  f(w) &= \sum_{i=1}^n \sum_{j=1}^n (a_i^\top w - b_j^\top w)^2 + \lambda \lVert w \rVert_2^2\\
  &= w^{\top} \left( \sum_{i = 1}^n \sum_{j = 1}^n (a_i - b_j)(a_i - b_j)^{\top} \right) w + \lambda w^{\top}w\\
  &= \left( \left(\sum_{i = 1}^n a_i - \sum_{j = 1}^n b_j\right)\left(\sum_{i = 1}^n a_i - \sum_{j = 1}^n b_j\right)^{\top} + \lambda \right) w^{\top}w
  \end{align*}

\end{enumerate}
\fi

\end{document}
